{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0448a1cf",
   "metadata": {},
   "source": [
    "# Symbolic Transformer\n",
    "## About the symbolic transformer\n",
    "The text discusses the use of the SymbolicTransformer to generate new non-linear features automatically. This implies that the SymbolicTransformer is used to transform the original dataset and create new features, which might help improve the performance of a machine learning model.\n",
    "\n",
    "In other words the symbolic transformer is transforming the feature space to improve the accuracy of the model. \n",
    "\n",
    "## About the data\n",
    "The text refers to the \"Diabetes housing dataset\" and mentions that Ridge Regression is used as the estimator. The dataset is divided into training and testing sets (300 samples for training and 200 for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6afd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicTransformer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "\n",
    "rng = check_random_state(0)\n",
    "diabetes = load_diabetes()\n",
    "perm = rng.permutation(diabetes.target.size)\n",
    "diabetes.data = diabetes.data[perm]\n",
    "diabetes.target = diabetes.target[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418d784",
   "metadata": {},
   "source": [
    "## About the model accuracy\n",
    "\n",
    "The initial benchmark to compare the performance of the Ridge regression model on the dataset is provided as an R2 score, which is approximately 0.4341.\n",
    "\n",
    "This R2 score is simply the result of the Ridge regression running on the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7850f8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434057421057894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "est = Ridge()\n",
    "est.fit(diabetes.data[:300, :], diabetes.target[:300])\n",
    "print(est.score(diabetes.data[300:, :], diabetes.target[300:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a64099",
   "metadata": {},
   "source": [
    "## About the sybolic transformation\n",
    "The SymbolicTransformer is trained on the first 300 samples of the dataset. It uses a population of 2000 individuals over 20 generations. From this population, the best 100 individuals are selected for the \"hall_of_fame.\" The text mentions that the \"least-correlated 10\" individuals are used as new features. This implies that the SymbolicTransformer generates new features, but not all of them are used. Instead, the 10 features with the least correlation with the existing features are selected.\n",
    "\n",
    "**Feature Generation:** The function_set for the SymbolicTransformer is specified, and parameters like parsimony coefficient, max_samples, and random_state are set. The transformation process is verbose, and three parallel jobs are used for processing.\n",
    "\n",
    "**Transformed Dataset:** The transformed features generated by the SymbolicTransformer are then concatenated with the original data, creating a new dataset called \"new_diabetes.\"\n",
    "\n",
    "**Model Performance with New Features:** A Ridge regression model is trained on the new dataset, and its performance is evaluated on the final 200 samples. The R2 score for this model is approximately 0.5337, indicating an improvement in performance compared to the initial benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affda0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
    "                'abs', 'neg', 'inv', 'max', 'min']\n",
    "gp = SymbolicTransformer(generations=20, population_size=2000,\n",
    "                         hall_of_fame=100, n_components=10,\n",
    "                         function_set=function_set,\n",
    "                         parsimony_coefficient=0.0005,\n",
    "                         max_samples=0.9, verbose=1,\n",
    "                         random_state=0)\n",
    "gp.fit(diabetes.data[:300, :], diabetes.target[:300])\n",
    "\n",
    "gp_features = gp.transform(diabetes.data)\n",
    "new_diabetes = np.hstack((diabetes.data, gp_features))\n",
    "\n",
    "est = Ridge()\n",
    "est.fit(new_diabetes[:300, :], diabetes.target[:300])\n",
    "print(est.score(new_diabetes[300:, :], diabetes.target[300:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325dd80",
   "metadata": {},
   "source": [
    "## Some important things to analyze\n",
    "\n",
    "**Increasing Variable Space:** Increasing the number of features (variables) in the model can lead to improved accuracy, especially if these new features capture previously unmodeled patterns or relationships in the data. However, there's a trade-off to consider. Increasing the dimensionality of the feature space can also lead to overfitting. Overfitting occurs when a model becomes too complex, capturing noise in the data rather than the underlying patterns. This can result in reduced model generalization and higher bias when the model is applied to new, unseen data. So, while adding more features can improve accuracy, it should be done carefully to avoid overfitting. However, overfitting is avoided by the use of the least correlated variables.\n",
    "\n",
    "**Using the Least Correlated Variables:** The text mentions selecting the least-correlated 10 new variables. This is a strategy to mitigate the risk of multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated with each other, making it challenging for the model to distinguish the individual effects of these variables. By choosing the least correlated among the generated features, the model aims to incorporate diverse and non-redundant information, reducing the risk of multicollinearity and, potentially, overfitting. It's a way to control the dimensionality of the model while maintaining the most valuable new information.\n",
    "\n",
    "**Hall_of_Fame:** I first to struggle on what hall of fame was, after some reading I can state that in genetic programming, it's a collection of the best-performing individuals (solutions) that have evolved through multiple generations of the algorithm (in this case 20). The hall_of_fame is typically used to preserve and propagate the most successful and promising solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299815b8",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Genetic programming is a type of evolutionary algorithm and a machine learning technique that is used to automatically evolve computer programs to perform a specific task or solve a problem. It is inspired by the process of natural selection and genetic evolution.\n",
    "\n",
    "Genetic programming can be applied to a wide range of problems, including symbolic regression, symbolic classification, automatic code generation. \n",
    "\n",
    "In the example of symbolic regression and the Boston housing dataset the symbolic regression turned out to be a better regression model than a typical ridge regression. This improvement in the performance is derived from the increase of the feature space to transform the variable space to the autamitcally selected features by the model (in this case the least 10 correlated variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cd655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
